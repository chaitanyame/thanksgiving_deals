services:
  # Initial import from Google Sheets
  import-sheet:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./data:/app/data
      - ./scripts:/app/scripts
    environment:
      - TZ=UTC
    command: python scripts/import_from_sheet_html.py
    restart: "no"
    networks:
      - deals-network

  # Python scraper service - merges RSS with sheet data
  scraper:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./data:/app/data
      - ./scripts:/app/scripts
    environment:
      - TZ=UTC
    command: python scripts/sync_combined.py
    depends_on:
      import-sheet:
        condition: service_completed_successfully
    restart: "no"  # Run once on docker-compose up
    networks:
      - deals-network

  # Nginx web server to host the static website
  web:
    image: nginx:alpine
    ports:
      - "8888:80"
    volumes:
      - ./index.html:/usr/share/nginx/html/index.html:ro
      - ./css:/usr/share/nginx/html/css:ro
      - ./js:/usr/share/nginx/html/js:ro
      - ./data:/usr/share/nginx/html/data:ro
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - scraper
    networks:
      - deals-network
    restart: unless-stopped

  # Optional: Scheduler service to run scraper periodically
  # Uncomment to enable automatic syncing every 30 minutes
  # scheduler:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   volumes:
  #     - ./data:/app/data
  #     - ./scripts:/app/scripts
  #   environment:
  #     - TZ=UTC
  #   command: >
  #     sh -c "while true; do 
  #       python scripts/sync_combined.py && 
  #       echo 'Sync completed. Waiting 30 minutes...' && 
  #       sleep 1800; 
  #     done"
  #   depends_on:
  #     - web
  #   networks:
  #     - deals-network
  #   restart: unless-stopped

networks:
  deals-network:
    driver: bridge
